import os
import shutil
import hashlib
import logging
import argparse
from pathlib import Path
from datetime import datetime
from PIL import Image, UnidentifiedImageError
import imagehash
import exifread
import sys
import concurrent.futures
from logging.handlers import RotatingFileHandler
import signal
import contextlib
import threading # å¼•å…¥ threading æ¨¡å—ç”¨äºé”
import re # Import the re module for regular expressions

# ===== é»˜è®¤é…ç½®ï¼ˆå¯è¢«å‚æ•°è¦†ç›–ï¼‰=====

HASH_ALGO = 'sha256'
HASH_THRESHOLD = 5
# Keep default for fallback/pattern matching, but will generate numbered files
DEFAULT_LOG_FILE = "photo_dedup.log"
# æ·»åŠ å¸¸è§è§†é¢‘æ ¼å¼ï¼Œä½†è¯·æ³¨æ„ç›¸ä¼¼åº¦åˆ¤æ–­ä»…å¯¹å›¾ç‰‡æœ‰æ•ˆ
IMAGE_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp', '.mp4', '.avi', '.mov', '.mkv']
DEFAULT_MIN_SIZE_KB = 100 # é»˜è®¤æœ€å°æ–‡ä»¶å¤§å°ä¸º 100 KB
GPS_THRESHOLD = 0.0001
LOG_FILE_SIZE_LIMIT_MB = 10

# ===== å…¨å±€ä¸­æ–­æ ‡å¿— =====

interrupted = False

# ===== ä¿¡å·å¤„ç†å‡½æ•° =====

def signal_handler(sig, frame):
    """å¤„ç† Ctrl+C ä¸­æ–­ä¿¡å·"""
    global interrupted
    print("\nâš ï¸ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)ã€‚æ­£åœ¨å°è¯•å®‰å…¨é€€å‡º...")
    interrupted = True
    # åœ¨è¿™é‡Œä¸ç›´æ¥é€€å‡ºæˆ–æš‚åœï¼Œè®©çº¿ç¨‹æ± æœ‰æœºä¼šå®Œæˆå½“å‰ä»»åŠ¡æˆ–å“åº”ä¸­æ–­æ ‡å¿—

# æ•è· SIGTSTP ä¿¡å·ï¼ˆCtrl+Zï¼‰ï¼Œå®ç°æš‚åœåŠŸèƒ½ï¼ˆä»…éƒ¨åˆ†ç³»ç»Ÿæ”¯æŒï¼‰
def sigtstp_handler(sig, frame):
    """å¤„ç† Ctrl+Z (SIGTSTP) ä¿¡å·ï¼Œæš‚åœç¨‹åº"""
    global interrupted
    print("\nâš ï¸ ç¨‹åºå·²æš‚åœã€‚æŒ‰ Enter é”®æ¢å¤æ‰§è¡Œï¼Œæˆ–æŒ‰ Ctrl+C é€€å‡º...")
    interrupted = True # æ ‡è®°ä¸­æ–­ï¼Œä½†åç»­ä¼šé€šè¿‡ input() æˆ– signal.pause() é˜»å¡
    # ä½¿ç”¨ input() é˜»å¡ä¸»çº¿ç¨‹ï¼Œç­‰å¾…ç”¨æˆ·äº¤äº’
    input()
    print("â–¶ï¸ ç¨‹åºæ¢å¤æ‰§è¡Œ...")
    interrupted = False # é‡ç½®ä¸­æ–­æ ‡å¿—ï¼Œç¨‹åºç»§ç»­

# ===== è®¾ç½®ä¿¡å·å¤„ç† =====
# æ³¨æ„ï¼šä¿¡å·å¤„ç†ä¸»è¦å½±å“ä¸»çº¿ç¨‹ã€‚å·¥ä½œçº¿ç¨‹çš„ä¸­æ–­éœ€è¦æ£€æŸ¥å…¨å±€ä¸­æ–­æ ‡å¿—
signal.signal(signal.SIGINT, signal_handler)  # æ•è· Ctrl+C
# signal.signal(signal.SIGTSTP, sigtstp_handler) # æ•è· Ctrl+Zï¼ˆSIGTSTPï¼‰ï¼Œåœ¨æŸäº›ç¯å¢ƒä¸‹å¯èƒ½å¹²æ‰°çº¿ç¨‹æ± 

# ===== å·¥å…·å‡½æ•° =====

def file_hash(filepath, algo=HASH_ALGO):
    """è®¡ç®—æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    h = hashlib.new(algo)
    try:
        with open(filepath, 'rb') as f:
            while chunk := f.read(8192):
                h.update(chunk)
        return h.hexdigest()
    except Exception as e:
        logging.error(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {filepath}ï¼ŒåŸå› : {e}")
        return None

def to_decimal_degrees(dms):
    """å°† EXIF çš„ DMS (åº¦åˆ†ç§’) æ ¼å¼è½¬æ¢ä¸ºåè¿›åˆ¶åº¦æ•°"""
    degrees = float(dms[0].num) / float(dms[0].den)
    minutes = float(dms[1].num) / float(dms[1].den)
    seconds = float(dms[2].num) / float(dms[2].den)
    return degrees + (minutes / 60.0) + (seconds / 3600.0)

def get_gps_coordinates(filepath):
    """ä»æ–‡ä»¶ EXIF ä¸­æå– GPS åæ ‡"""
    try:
        with open(filepath, 'rb') as f:
            try:
                # Process only necessary tags up to GPS info for efficiency
                # Changed stop_tag to be more comprehensive
                tags = exifread.process_file(f, stop_tag="GPS GPSLongitude", details=False)

                # Check if GPS tags are present
                if 'GPS GPSLatitude' in tags and 'GPS GPSLongitude' in tags:
                    try:
                        lat_val = tags['GPS GPSLatitude'].values
                        lon_val = tags['GPS GPSLongitude'].values
                        latitude = to_decimal_degrees(lat_val)
                        longitude = to_decimal_degrees(lon_val)
                        lat_ref = tags.get('GPS GPSLatitudeRef')
                        lon_ref = tags.get('GPS GPSLongitudeRef')

                        if lat_ref and lat_ref.values[0] == 'S':
                            latitude = -latitude
                        if lon_ref and lon_ref.values[0] == 'W':
                            longitude = -longitude
                        return latitude, longitude
                    # Added IndexError to catch potential issues with values list
                    except (AttributeError, TypeError, IndexError) as e:
                        logging.warning(f"âš ï¸ è§£æ GPS åæ ‡å€¼å¤±è´¥: {filepath}ï¼ŒåŸå› : {e}")
                        return None
                # It's also possible to have one without the other, although less useful
                elif 'GPS GPSLatitude' in tags or 'GPS GPSLongitude' in tags:
                     logging.warning(f"âš ï¸ GPS åæ ‡ä¿¡æ¯ä¸å®Œæ•´ (ä»…å‘ç°Latitudeæˆ–Longitude): {filepath}")
                     return None
                else:
                    # No GPS tags found
                    return None
            # Catch the specific EXIFError from the submodule
            except exifread.exceptions.EXIFError as e: # <--- CHANGED THIS LINE
                logging.warning(f"âš ï¸ è¯»å– EXIF ä¿¡æ¯å¤±è´¥ (GPS ç›¸å…³): {filepath}ï¼ŒåŸå› : {e}")
                return None
    except FileNotFoundError:
        # This might happen if the file is deleted by another thread between finding it and processing
        logging.debug(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è¯»å–æˆ–å¤„ç† GPS ä¿¡æ¯: {filepath}") # Use debug level as it's expected in concurrent runs
        return None
    except Exception as e:
        # Catch other potential errors during file open or initial processing
        logging.error(f"âŒ æ‰“å¼€æ–‡ä»¶æˆ–å¤„ç† GPS ä¿¡æ¯å¤±è´¥: {filepath}ï¼ŒåŸå› : {e}")
        return None

def compare_gps(coord1, coord2, threshold=GPS_THRESHOLD):
    """æ¯”è¾ƒä¸¤ä¸ª GPS åæ ‡æ˜¯å¦åœ¨é˜ˆå€¼èŒƒå›´å†…"""
    if coord1 is None or coord2 is None:
        return False
    lat1, lon1 = coord1
    lat2, lon2 = coord2
    return abs(lat1 - lat2) < threshold and abs(lon1 - lon2) < threshold

def is_image_file(filepath):
    """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ˜¯ PIL å¯ä»¥æ‰“å¼€çš„å›¾ç‰‡æ–‡ä»¶"""
    # Only check common image extensions first for efficiency
    if filepath.suffix.lower() not in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']:
         return False
    try:
        with Image.open(filepath) as img:
            img.verify() # Verify file integrity
        return True
    except Exception: # Catch all exceptions, including UnidentifiedImageError, IOError, etc.
        return False

def get_image_resolution(filepath):
    """è·å–å›¾ç‰‡çš„åˆ†è¾¨ç‡ (å®½åº¦ * é«˜åº¦)"""
    if not is_image_file(filepath): # Only try for recognized image files
        return 0
    try:
        with Image.open(filepath) as img:
            return img.size[0] * img.size[1]
    except UnidentifiedImageError as e:
        logging.warning(f"âš ï¸ æ— æ³•è¯†åˆ«çš„å›¾ç‰‡æ ¼å¼ï¼Œæ— æ³•è·å–åˆ†è¾¨ç‡: {filepath}ï¼ŒåŸå› : {e}")
        return 0
    except FileNotFoundError:
        # This might happen if the file is deleted by another thread
        logging.debug(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è·å–åˆ†è¾¨ç‡: {filepath}")
        return 0
    except Exception as e:
        logging.error(f"âŒ è·å–å›¾ç‰‡åˆ†è¾¨ç‡å¤±è´¥: {filepath}ï¼ŒåŸå› : {e}")
        return 0

def backup_file(file_path, perform_actions, backup_dir, source_dir_arg, simple_backup=False, simple_backup_with_path=False, reason="", overwrite_files=False):
    """å¤‡ä»½æ–‡ä»¶åˆ°æŒ‡å®šç›®å½•"""
    if not perform_actions:
        log_message(f"å¤‡ä»½: {file_path} -> {backup_dir} (æ¨¡æ‹Ÿ)", False, False)
        return

    original_name = file_path.stem.replace(" ", "_")
    ext = file_path.suffix.lower()
    # ä½¿ç”¨æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´åˆ›å»ºæ—¥æœŸç›®å½•
    try:
        timestamp_dir_name = datetime.fromtimestamp(file_path.stat().st_mtime).strftime('%Y-%m-%d')
        timestamp_dir = backup_dir / timestamp_dir_name
    except Exception as e:
         logging.error(f"âŒ è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´å¤±è´¥: {file_path}ï¼ŒåŸå› : {e}. ä½¿ç”¨å½“å‰æ—¥æœŸä»£æ›¿ã€‚")
         timestamp_dir_name = datetime.now().strftime('%Y-%m-%d_error')
         timestamp_dir = backup_dir / timestamp_dir_name


    # ç¡®å®šå¤‡ä»½è·¯å¾„
    target_dir = timestamp_dir
    if simple_backup_with_path:
        # æ„å»ºç›¸å¯¹äºæºç›®å½•çš„è·¯å¾„
        try:
            relative_path = file_path.relative_to(source_dir_arg).parent
            target_dir = timestamp_dir / relative_path
        except ValueError:
            # å¦‚æœæ–‡ä»¶ä¸åœ¨æºç›®å½•ä¸‹ (ä¾‹å¦‚æ¥è‡ªå¯é€‰ç›®å½•)ï¼Œåˆ™ç›´æ¥æ”¾åœ¨æ—¥æœŸç›®å½•ä¸‹
            logging.warning(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸åœ¨æºç›®å½• {source_dir_arg} ä¸‹ï¼Œæ— æ³•æ„å»ºç›¸å¯¹è·¯å¾„å¤‡ä»½ã€‚å¤‡ä»½åˆ° {timestamp_dir}")
            target_dir = timestamp_dir
        except Exception as e:
            logging.error(f"âŒ æ„å»ºç›¸å¯¹å¤‡ä»½è·¯å¾„å¤±è´¥: {file_path}ï¼ŒåŸå› : {e}. å¤‡ä»½åˆ° {timestamp_dir}")
            target_dir = timestamp_dir

    # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
    try:
         target_dir.mkdir(parents=True, exist_ok=True)
    except Exception as e:
         logging.error(f"âŒ åˆ›å»ºå¤‡ä»½ç›®å½•å¤±è´¥: {target_dir}ï¼ŒåŸå› : {e}")
         return # å¦‚æœç›®å½•æ— æ³•åˆ›å»ºï¼Œåˆ™è·³è¿‡å¤‡ä»½

    # ç¡®å®šæœ€ç»ˆæ–‡ä»¶å
    if simple_backup:
        # ç®€å•æ¨¡å¼ï¼šåŸå§‹æ–‡ä»¶å
        backup_path = target_dir / f"{original_name}{ext}"
        # åœ¨ç®€å•æ¨¡å¼ä¸‹ï¼Œå¦‚æœåŒåæ–‡ä»¶å·²å­˜åœ¨ï¼Œæˆ‘ä»¬ä¸åšç‰¹æ®Šå¤„ç†ï¼Œè®© copy2 å†³å®šæ˜¯å¦è¦†ç›–ï¼ˆå–å†³äº overwrite_filesï¼‰
        # å¦‚æœéœ€è¦åŒºåˆ†åŒåæ–‡ä»¶ï¼Œåº”ä½¿ç”¨ -s1 æˆ–é»˜è®¤æ¨¡å¼
    elif simple_backup_with_path:
        # åŸå§‹è·¯å¾„ + åŸå§‹æ–‡ä»¶å
        backup_path = target_dir / f"{original_name}{ext}"
    else:
        # é»˜è®¤æ¨¡å¼ï¼šåŸå§‹æ–‡ä»¶å + åŸå›  + å“ˆå¸Œ
        file_hash_name = file_hash(file_path)
        if file_hash_name is None:
             logging.error(f"âŒ æ— æ³•è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œè·³è¿‡å¤‡ä»½: {file_path}")
             return
        suffix = f"_{reason}" if reason else ""
        new_file_name = f"{original_name}{suffix}_{file_hash_name[:8]}{ext}" # ä½¿ç”¨å“ˆå¸Œå‰8ä½é¿å…æ–‡ä»¶åè¿‡é•¿
        backup_path = target_dir / new_file_name

    # å¦‚æœå¤‡ä»½è·¯å¾„å·²å­˜åœ¨ï¼Œæ ¹æ® overwrite_files é€‰æ‹©æ˜¯å¦è¦†ç›–
    if backup_path.exists():
        if overwrite_files:
            logging.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè¦†ç›–: {backup_path}")
        else:
            logging.info(f"å¤‡ä»½æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡å¤‡ä»½: {backup_path}")
            return

    # å¤‡ä»½æ–‡ä»¶
    try:
        shutil.copy2(file_path, backup_path)
        log_message(f"å·²å¤‡ä»½: {file_path} â†’ {backup_path}", True, perform_actions)
    except Exception as e:
        logging.error(f"âŒ å¤‡ä»½æ–‡ä»¶å¤±è´¥: {file_path} åˆ° {backup_path}ï¼ŒåŸå› : {e}")

def log_message(message, enable_console_log, is_executing):
    """æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åŒæ—¶è¾“å‡ºæ—¥å¿—åˆ°æ§åˆ¶å°"""
    # æ—¥å¿—æ ¼å¼å·²åœ¨ main ä¸­é…ç½®ï¼Œè¿™é‡Œåªè´Ÿè´£è°ƒç”¨ logging.info
    logging.info(message)
    # åªæœ‰å½“ enable_console_log ä¸º True æ—¶æ‰æ‰“å°åˆ°æ§åˆ¶å°
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†ä½¿ç”¨ print ç›´æ¥è¾“å‡ºï¼Œè€Œæ˜¯ä¾èµ–äº logging handler
    # å¦‚æœ logging é…ç½®ä¸­åŒ…å«äº† StreamHandler å¹¶ä¸” enable_console_log ä¸º Trueï¼Œæ¶ˆæ¯ä¼šè‡ªåŠ¨è¾“å‡ºåˆ°æ§åˆ¶å°
    pass # Remove direct print here

# Modified log_message to use logging levels and ensure console output via handler
def log_action(level, message, enable_console_log):
    """Log a message with a specific level, optionally printing to console."""
    logger = logging.getLogger() # Get the root logger

    # Check if a console handler is already attached
    has_console_handler = any(isinstance(h, logging.StreamHandler) for h in logger.handlers)

    # If console output is enabled and no console handler exists, add one temporarily
    # This ensures messages are printed even if the main console handler setup is complex
    # However, it's better to rely on the main setup in __main__
    # Simpler approach: just log, and the main setup handles console output
    if level == logging.INFO:
        logging.info(message)
    elif level == logging.WARNING:
        logging.warning(message)
    elif level == logging.ERROR:
        logging.error(message)
    elif level == logging.DEBUG:
        logging.debug(message)
    else:
        logging.info(message) # Default to info

    # No need for explicit print here if StreamHandler is configured in __main__
    # If enable_console_log is True, the StreamHandler added in __main__ will handle it.
    pass


def safe_delete_file(file_path, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=False, simple_backup_with_path=False, reason=""):
    """å®‰å…¨åˆ é™¤æ–‡ä»¶ï¼Œå¯é€‰å¤‡ä»½æˆ–è½¯åˆ é™¤"""
    # Use log_action for messages that might interfere with progress bar
    if not perform_actions:
        log_action(logging.INFO, f"[åˆ é™¤]: {file_path} (æ¨¡æ‹Ÿ)", enable_console_log)
        return True # æ¨¡æ‹Ÿåˆ é™¤æˆåŠŸ

    # åœ¨æ‰§è¡Œåˆ é™¤å‰ï¼Œå…ˆæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢é‡å¤åˆ é™¤æˆ–åˆ é™¤ä¸å­˜åœ¨çš„æ–‡ä»¶
    if not file_path.exists():
        log_action(logging.WARNING, f"âš ï¸ å°è¯•åˆ é™¤æ–‡ä»¶ {file_path}ï¼Œä½†æ–‡ä»¶ä¸å­˜åœ¨ã€‚è·³è¿‡åˆ é™¤ã€‚", enable_console_log)
        return False # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤

    deleted_successfully = False
    if delete_soft and trash_dir:
        # è½¯åˆ é™¤ (ç§»åŠ¨åˆ°å›æ”¶ç«™ç›®å½•)
        trash_path = Path(trash_dir) / file_path.name # ç®€å•åœ°ç§»åŠ¨åˆ°å›æ”¶ç«™æ ¹ç›®å½•ï¼Œå¯èƒ½ä¼šæœ‰æ–‡ä»¶åå†²çª
        # æ›´å¥½çš„è½¯åˆ é™¤æ–¹å¼æ˜¯ä¿ç•™éƒ¨åˆ†åŸç›®å½•ç»“æ„ï¼Œæˆ–è€…åœ¨æ–‡ä»¶åååŠ æ—¶é—´æˆ³/å“ˆå¸Œ
        # ä¸ºäº†ç®€å•ï¼Œè¿™é‡Œåªç§»åŠ¨åˆ°æ ¹ç›®å½•ï¼Œä½†å¦‚æœæ–‡ä»¶åå†²çªï¼Œéœ€è¦å¤„ç†ï¼ˆæ¯”å¦‚åŠ åç¼€ï¼‰
        # shutil.move å¦‚æœç›®æ ‡å­˜åœ¨ä¸”ä¸æ˜¯ç›®å½•ï¼Œä¼šè¦†ç›–ã€‚å¦‚æœæ˜¯ç›®å½•ï¼Œä¼šç§»åŠ¨åˆ°ç›®å½•ä¸‹ã€‚
        # å¦‚æœç›®æ ‡è·¯å¾„å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼Œè¿™é‡Œä¼šæŠ›å‡º FileExistsError æˆ– IsADirectoryError
        try:
            # å¦‚æœç›®æ ‡å·²å­˜åœ¨åŒåæ–‡ä»¶ï¼ŒåŠ ä¸ªåç¼€
            if trash_path.exists():
                name, ext = os.path.splitext(trash_path.name)
                timestamp_suffix = datetime.now().strftime('_%Y%m%d%H%M%S')
                trash_path = trash_path.with_name(f"{name}{timestamp_suffix}{ext}")
                log_action(logging.WARNING, f"âš ï¸ å›æ”¶ç«™å·²å­˜åœ¨åŒåæ–‡ä»¶ {file_path.name}ï¼Œç§»åŠ¨åˆ° {trash_path}", enable_console_log)

            # ç¡®ä¿å›æ”¶ç«™ç›®å½•å­˜åœ¨
            trash_path.parent.mkdir(parents=True, exist_ok=True)

            shutil.move(str(file_path), str(trash_path))
            log_action(logging.INFO, f"[è½¯åˆ é™¤] å·²ç§»åŠ¨åˆ° {trash_path}: {file_path}", enable_console_log)
            deleted_successfully = True
        except Exception as e:
            log_action(logging.ERROR, f"âŒ [è½¯åˆ é™¤] ç§»åŠ¨æ–‡ä»¶å¤±è´¥ {file_path} åˆ° {trash_path}: {e}", enable_console_log)
            deleted_successfully = False # ç§»åŠ¨å¤±è´¥ï¼Œè§†ä¸ºåˆ é™¤å¤±è´¥
    else: # é»˜è®¤è¡Œä¸ºï¼šå…ˆå¤‡ä»½å†ç¡¬åˆ é™¤
        # å¤‡ä»½æ–‡ä»¶
        # åœ¨æ‰§è¡Œåˆ é™¤å‰è°ƒç”¨å¤‡ä»½
        backup_file(file_path, perform_actions, backup_dir, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason=reason)

        # æ‰§è¡Œç¡¬åˆ é™¤
        try:
            file_path.unlink() # ç¡¬åˆ é™¤
            log_action(logging.INFO, f"[åˆ é™¤] (å¤‡ä»½å·²å®Œæˆ): {file_path}", enable_console_log)
            deleted_successfully = True
        except Exception as e:
            log_action(logging.ERROR, f"âŒ ç¡¬åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}", enable_console_log)
            deleted_successfully = False # åˆ é™¤å¤±è´¥

    return deleted_successfully # è¿”å›æ˜¯å¦åˆ é™¤æˆåŠŸ

def calculate_phash(filepath, cache, phash_cache_lock):
    """è®¡ç®—å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œï¼Œå¸¦ç¼“å­˜å’Œçº¿ç¨‹é”"""
    with phash_cache_lock:
        if filepath in cache:
            return cache[filepath]

    # å¦‚æœä¸åœ¨ç¼“å­˜ä¸­ï¼Œè®¡ç®—å¹¶å­˜å…¥
    phash_val = None
    if not is_image_file(filepath): # åªæœ‰å›¾ç‰‡æ‰èƒ½è®¡ç®—phash
         logging.debug(f"è·³è¿‡æ–‡ä»¶ {filepath} çš„æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—ï¼Œå› ä¸ºå®ƒä¸æ˜¯å›¾ç‰‡ã€‚")
         return None

    try:
        with Image.open(filepath) as img:
            phash_val = imagehash.phash(img.convert('RGB'))
        with phash_cache_lock: # å†æ¬¡è·å–é”ï¼Œç¡®ä¿å†™å…¥ç¼“å­˜æ˜¯çº¿ç¨‹å®‰å…¨çš„
             cache[filepath] = phash_val
    except UnidentifiedImageError as e:
        logging.warning(f"âš ï¸ æ— æ³•è¯†åˆ«çš„å›¾ç‰‡æ ¼å¼ï¼Œæ— æ³•è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ: {filepath}ï¼ŒåŸå› : {e}")
    except FileNotFoundError:
        # This might happen if the file is deleted by another thread
        logging.debug(f"æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ— æ³•è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ: {filepath}")
    except Exception as e:
        logging.error(f"âŒ è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œå¤±è´¥: {filepath}ï¼ŒåŸå› : {e}")

    return phash_val

# Modified to return whether the *current* file being processed (file) was deleted
# and to handle updating phash_list if the original was deleted
def handle_similar_images(file, original_file, file_phash, original_phash, args, source_dir_arg, phash_list, phash_list_lock):
    """å¤„ç†ç›¸ä¼¼å›¾ç‰‡å¯¹ï¼Œæ ¹æ®è§„åˆ™å†³å®šåˆ é™¤å“ªä¸ª"""
    # logging.debug(f"æ¯”è¾ƒç›¸ä¼¼å›¾ç‰‡: {file} å’Œ {original_file}") # è¿™æ¡æ—¥å¿—å¯èƒ½è¿‡äºé¢‘ç¹

    # Check if files still exist
    if not file.exists() or not original_file.exists():
        # One or both files might have been deleted by another thread (e.g. exact duplicate)
        logging.debug(f"ç›¸ä¼¼æ–‡ä»¶ {file} æˆ– {original_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡ç›¸ä¼¼æ€§æ¯”è¾ƒå¤„ç†ã€‚")
        return False # å½“å‰æ–‡ä»¶æœªè¢«åˆ é™¤ï¼Œä¹Ÿä¸å½±å“ä¿ç•™è®¡æ•°ï¼Œå› ä¸ºå¦ä¸€ä¸ªæ–‡ä»¶å¯èƒ½å·²è¢«åˆ é™¤å¹¶è®¡å…¥

    perform_actions = args.perform_actions
    backup_dir = Path(args.backup_dir)
    delete_soft = args.delete_soft
    trash_dir = Path(args.trash_dir) if args.trash_dir else None
    enable_console_log = args.log
    simple_backup = args.simple_backup
    simple_backup_with_path = args.simple_backup_path # Use corrected parameter name
    prefer_resolution = args.prefer_resolution

    gps_file = get_gps_coordinates(file)
    gps_orig = get_gps_coordinates(original_file)

    file_deleted = False # Flag to indicate if the current file ('file') was deleted

    # ä¼˜å…ˆçº§1: å«æœ‰GPSä¿¡æ¯
    if gps_file is not None and gps_orig is None:
        # Current file has GPS, original doesn't -> keep current, delete original
        safe_delete_file(original_file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="gps")
        logging.info(f"[ç›¸ä¼¼] ä¿ç•™å«GPS: {file}, åˆ é™¤: {original_file}")
        # Original was deleted, current was kept. Need to update phash_list later if original was the list entry.
        file_deleted = False

    elif gps_file is None and gps_orig is not None:
        # Original has GPS, current doesn't -> keep original, delete current
        safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="gps")
        logging.info(f"[ç›¸ä¼¼] ä¿ç•™å«GPS: {original_file}, åˆ é™¤: {file}")
        file_deleted = True # Current file was deleted

    # ä¼˜å…ˆçº§2: æ¯”è¾ƒåˆ†è¾¨ç‡æˆ–å¤§å° (å¦‚æœGPSä¿¡æ¯ç›¸åŒæˆ–éƒ½æ²¡æœ‰)
    else:
        # Both have GPS (and maybe similar location) or neither has GPS
        if prefer_resolution:
            res_file = get_image_resolution(file)
            res_orig = get_image_resolution(original_file)
            size_file = file.stat().st_size if file.exists() else 0 # Check exists before stat
            size_orig = original_file.stat().st_size if original_file.exists() else 0 # Check exists before stat

            if res_file > res_orig:
                # Current file has higher resolution -> keep current, delete original
                safe_delete_file(original_file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="resolution")
                logging.info(f"[ç›¸ä¼¼] ä¿ç•™åˆ†è¾¨ç‡æ›´é«˜: {file}, åˆ é™¤: {original_file}")
                file_deleted = False
            elif res_file < res_orig:
                # Original file has higher resolution -> keep original, delete current
                safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="resolution")
                logging.info(f"[ç›¸ä¼¼] ä¿ç•™: {original_file} (åˆ†è¾¨ç‡æ›´é«˜), åˆ é™¤: {file}")
                file_deleted = True # Current file was deleted
            else: # Resolution is the same, compare size
                if size_file > size_orig:
                    # Current file is larger -> keep current, delete original
                    safe_delete_file(original_file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="larger")
                    logging.info(f"[ç›¸ä¼¼] ä¿ç•™æ–‡ä»¶è¾ƒå¤§: {file}, åˆ é™¤: {original_file}")
                    file_deleted = False
                elif size_file < size_orig:
                    # Original file is larger -> keep original, delete current
                    safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="larger")
                    logging.info(f"[ç›¸ä¼¼] ä¿ç•™: {original_file} (æ–‡ä»¶è¾ƒå¤§), åˆ é™¤: {file}")
                    file_deleted = True # Current file was deleted
                else:
                    # Resolution and Size are the same, keep the original one encountered first (original_file)
                    safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="similar")
                    logging.info(f"[ç›¸ä¼¼] ä¿ç•™: {original_file}, åˆ é™¤: {file} (å¤§å°ç›¸åŒ)")
                    file_deleted = True # Current file was deleted

        else: # Not preferring resolution, just compare size
            size_file = file.stat().st_size if file.exists() else 0
            size_orig = original_file.stat().st_size if original_file.exists() else 0

            if size_file > size_orig:
                # Current file is larger -> keep current, delete original
                safe_delete_file(original_file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="larger")
                logging.info(f"[ç›¸ä¼¼] ä¿ç•™æ–‡ä»¶è¾ƒå¤§: {file}, åˆ é™¤: {original_file}")
                file_deleted = False
            elif size_file < size_orig:
                # Original file is larger -> keep original, delete current
                safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="larger")
                logging.info(f"[ç›¸ä¼¼] ä¿ç•™: {original_file} (æ–‡ä»¶è¾ƒå¤§), åˆ é™¤: {file}")
                file_deleted = True # Current file was deleted
            else:
                # Size is the same, keep the original one encountered first (original_file)
                safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="similar")
                logging.info(f"[ç›¸ä¼¼] ä¿ç•™: {original_file}, åˆ é™¤: {file} (å¤§å°ç›¸åŒ)")
                file_deleted = True # Current file was deleted

    # After deciding and potentially deleting, update phash_list if the original entry was deleted
    # This part needs the lock
    if not file_deleted: # If the current file was NOT deleted, it means the original_file WAS deleted (or skipped)
         # We should remove the original_file from phash_list and potentially add the current file
         # Add the current file as the representative if it wasn't already in the list
         # This is handled in process_file where it checks if the current file was deleted.
         # If file_deleted is False, process_file will add the current file to phash_list later if needed.
         pass

    # Return True if the CURRENT file being processed was deleted, False otherwise
    return file_deleted

# Modified to return whether the *current* file being processed (file) was deleted
def handle_exact_duplicate(file, original, args, source_dir_arg):
    """å¤„ç†å®Œå…¨é‡å¤çš„æ–‡ä»¶å¯¹ï¼Œæ ¹æ®è§„åˆ™å†³å®šåˆ é™¤å“ªä¸ª"""
    # logging.debug(f"å¤„ç†å®Œå…¨é‡å¤æ–‡ä»¶: {file} å’Œ {original}") # è¿™æ¡æ—¥å¿—å¯èƒ½è¿‡äºé¢‘ç¹

    # Check if files still exist
    if not file.exists() or not original.exists():
         logging.debug(f"é‡å¤æ–‡ä»¶ {file} æˆ– {original} ä¸å­˜åœ¨ï¼Œè·³è¿‡é‡å¤å¤„ç†ã€‚")
         return False # å½“å‰æ–‡ä»¶æœªè¢«åˆ é™¤

    # If the file path is exactly the same, it's not a duplicate pair to handle here
    if file == original:
         logging.debug(f"æ–‡ä»¶ {file} æ˜¯è‡ªèº«å¼•ç”¨ï¼Œè·³è¿‡é‡å¤å¤„ç†ã€‚")
         return False # å½“å‰æ–‡ä»¶æœªè¢«åˆ é™¤

    perform_actions = args.perform_actions
    backup_dir = Path(args.backup_dir)
    delete_soft = args.delete_soft
    trash_dir = Path(args.trash_dir) if args.trash_dir else None
    enable_console_log = args.log
    simple_backup = args.simple_backup
    simple_backup_with_path = args.simple_backup_path # Use corrected parameter name

    gps_file = get_gps_coordinates(file)
    gps_original = get_gps_coordinates(original)

    file_deleted = False # Flag to indicate if the current file ('file') was deleted

    # ä¼˜å…ˆçº§1: å«æœ‰GPSä¿¡æ¯
    if gps_file is not None and gps_original is None:
        # Current file has GPS, original doesn't -> keep current, delete original
        # In exact duplicates, we usually keep the 'original' one found first (the one in seen_hashes)
        # So, if the 'original' in seen_hashes *doesn't* have GPS but the 'current' one *does*, we should delete the 'original' and update seen_hashes
        # This is complex with threading. A simpler rule is to keep the 'original' in seen_hashes if it has GPS, otherwise delete the current one if it has GPS.
        # Let's stick to the rule: keep the one with GPS. If only current has GPS, keep current, delete original.
        # This is slightly different logic for exact duplicates vs similar duplicates, but reasonable.
         safe_delete_file(original, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="gps_duplicate")
         logging.info(f"[é‡å¤] ä¿ç•™å«GPS: {file}, åˆ é™¤: {original}")
         file_deleted = False # Original was deleted

    elif gps_file is None and gps_original is not None:
        # Original has GPS, current doesn't -> keep original, delete current
        safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="gps_duplicate")
        logging.info(f"[é‡å¤] ä¿ç•™å«GPS: {original}, åˆ é™¤: {file}")
        file_deleted = True # Current file was deleted

    # ä¼˜å…ˆçº§2: å¦‚æœGPSä¿¡æ¯ç›¸åŒæˆ–éƒ½æ²¡æœ‰ -> é»˜è®¤ä¿ç•™ seen_hashes ä¸­çš„åŸæ–‡ä»¶ï¼Œåˆ é™¤å½“å‰æ–‡ä»¶
    else:
        # Both have GPS (and possibly same location) or neither has GPS
        # Keep the original one that was recorded first (original in seen_hashes)
        safe_delete_file(file, perform_actions, backup_dir, delete_soft, trash_dir, enable_console_log, source_dir_arg, simple_backup=simple_backup, simple_backup_with_path=simple_backup_with_path, reason="duplicate")
        logging.info(f"[é‡å¤] ä¿ç•™: {original}, åˆ é™¤: {file}")
        file_deleted = True # Current file was deleted

    # Return True if the CURRENT file being processed was deleted, False otherwise
    return file_deleted


# ===== æ ¸å¿ƒæ–‡ä»¶å¤„ç†å‡½æ•° (åœ¨çº¿ç¨‹ä¸­è¿è¡Œ) =====
def process_file(file, seen_hashes, phash_cache, phash_list, args, source_dir_arg,
                 seen_hashes_lock, phash_cache_lock, phash_list_lock):
    """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ŒæŸ¥æ‰¾é‡å¤æˆ–ç›¸ä¼¼æ–‡ä»¶ï¼Œå¹¶æ ¹æ®è§„åˆ™è¿›è¡Œæ“ä½œ"""
    global interrupted

    # æ£€æŸ¥å…¨å±€ä¸­æ–­æ ‡å¿—
    if interrupted:
        logging.info(f"ğŸ›‘ çº¿ç¨‹æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†æ–‡ä»¶: {file}")
        return 0 # å› ä¸­æ–­è·³è¿‡ï¼Œæœªåˆ é™¤æ–‡ä»¶

    # å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢æ–‡ä»¶åœ¨æ‰«æåå’Œå¤„ç†å‰è¢«åˆ é™¤
    if not file.exists():
        logging.debug(f"æ–‡ä»¶ {file} ä¸å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†ã€‚")
        return 0

    try:
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = file.stat().st_size
        min_size_bytes = args.min_size * 1024
        if file_size < min_size_bytes:
             logging.debug(f"æ–‡ä»¶ {file} å°äºæœ€å°æ‰«æå¤§å° ({args.min_size} KB)ï¼Œè·³è¿‡ã€‚")
             return 0

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åæ˜¯å¦åœ¨å…è®¸èŒƒå›´å†…
        # æ³¨æ„ï¼šprocess_directory å·²ç»è¿‡æ»¤äº†ä¸€æ¬¡ï¼Œä½†è¿™é‡Œå¯ä»¥ä½œä¸ºäºŒæ¬¡ç¡®è®¤æˆ–é’ˆå¯¹ç‰¹å®šå¤„ç†æ­¥éª¤çš„è¿‡æ»¤
        if file.suffix.lower() not in IMAGE_EXTENSIONS:
             logging.debug(f"æ–‡ä»¶ {file} æ‰©å±•åä¸åœ¨å…è®¸èŒƒå›´ {IMAGE_EXTENSIONS} å†…ï¼Œè·³è¿‡ã€‚")
             # å¯¹äºéå…è®¸æ‰©å±•åçš„æ–‡ä»¶ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†ï¼ˆä¸åˆ é™¤ä¹Ÿä¸å¤‡ä»½ï¼‰
             return 0


        # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¹¶æ£€æŸ¥å®Œå…¨é‡å¤æ–‡ä»¶
        file_hash_val = file_hash(file)
        if file_hash_val is None:
             logging.error(f"âŒ æ— æ³•è®¡ç®—å“ˆå¸Œï¼Œè·³è¿‡æ–‡ä»¶: {file}")
             return 0 # å“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œä»»ä½•å¤„ç†

        is_exact_duplicate = False
        original_exact_file = None
        with seen_hashes_lock:
            if file_hash_val in seen_hashes:
                original_exact_file = seen_hashes[file_hash_val]
                is_exact_duplicate = True
            else:
                # å¦‚æœä¸æ˜¯å®Œå…¨é‡å¤æ–‡ä»¶ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œç¬¬ä¸€æ¬¡è§ï¼‰ï¼Œå°†å…¶å“ˆå¸Œæ·»åŠ åˆ° seen_hashes ä¸­
                seen_hashes[file_hash_val] = file

        if is_exact_duplicate:
            # æ‰¾åˆ°äº†å®Œå…¨é‡å¤æ–‡ä»¶
            # original_exact_file æ˜¯ seen_hashes ä¸­è®°å½•çš„ç¬¬ä¸€ä¸ªæ‹¥æœ‰æ­¤å“ˆå¸Œçš„æ–‡ä»¶è·¯å¾„
            if original_exact_file.exists() and file != original_exact_file:
                 # å¤„ç†å®Œå…¨é‡å¤å¯¹ï¼Œå¹¶æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦è¢«åˆ é™¤
                 # handle_exact_duplicate è¿”å› True å¦‚æœå½“å‰æ–‡ä»¶è¢«åˆ é™¤
                 file_was_deleted = handle_exact_duplicate(file, original_exact_file, args, source_dir_arg)
                 if file_was_deleted:
                     return 1 # å½“å‰æ–‡ä»¶è¢«åˆ é™¤äº†
                 else:
                     # å¦‚æœå½“å‰æ–‡ä»¶å› ä¸º GPS ç­‰è§„åˆ™è¢«ä¿ç•™äº† (åŸæ–‡ä»¶è¢«åˆ é™¤)
                     # ç†è®ºä¸Šå®ƒç°åœ¨æ˜¯ seen_hashes ä¸­è¿™ä¸ªå“ˆå¸Œå¯¹åº”çš„æ›´å¥½çš„æ–‡ä»¶ï¼Œä½†æˆ‘ä»¬ä¸ä¿®æ”¹ seen_hashes ä»¥ç®€åŒ–å¹¶å‘é€»è¾‘
                     # è¿™ç§æƒ…å†µä¸‹å½“å‰æ–‡ä»¶æœªè¢«åˆ é™¤
                     return 0
            elif file == original_exact_file:
                 # æ–‡ä»¶è·¯å¾„å®Œå…¨ç›¸åŒï¼Œè¿™æ˜¯åŒä¸€ä¸ªæ–‡ä»¶ï¼Œä¸å¤„ç†
                 logging.debug(f"æ–‡ä»¶ {file} æ˜¯è‡ªèº«å¼•ç”¨ï¼Œè·³è¿‡é‡å¤æ£€æŸ¥ã€‚")
                 return 0
            else:
                 # original_exact_file ä¸å­˜åœ¨ (å¯èƒ½å·²è¢«å…¶ä»–çº¿ç¨‹å¤„ç†å¹¶åˆ é™¤)ï¼Œé‚£ä¹ˆå½“å‰æ–‡ä»¶ä¹Ÿæ˜¯é‡å¤çš„ï¼Œä½†æ²¡æœ‰åŸä»¶å¯ä»¥æ¯”å¯¹ã€‚
                 # è¿™é‡Œé€‰æ‹©è·³è¿‡å¤„ç†å½“å‰æ–‡ä»¶ï¼Œè®¤ä¸ºå®ƒå¯èƒ½å·²è¢«å…¶ä»–é€»è¾‘å¤„ç†æˆ–ä¸åº”å¤„ç†ã€‚
                 logging.debug(f"æ–‡ä»¶ {file} æ˜¯å“ˆå¸Œé‡å¤æ–‡ä»¶ï¼Œä½†åŸæ–‡ä»¶ {original_exact_file} ä¸å­˜åœ¨ï¼Œè·³è¿‡åˆ é™¤ã€‚")
                 return 0 # æœªåˆ é™¤å½“å‰æ–‡ä»¶

        # 3. å¦‚æœä¸æ˜¯å®Œå…¨é‡å¤ï¼Œæ£€æŸ¥ç›¸ä¼¼å›¾ç‰‡ (ä»…å¯¹å›¾ç‰‡ä¸”å¯ç”¨ç›¸ä¼¼åº¦æ£€æŸ¥å’Œå»é‡æ—¶)
        # æ³¨æ„ï¼šè§†é¢‘æ–‡ä»¶ä¸å‚ä¸ç›¸ä¼¼åº¦æ£€æŸ¥
        is_img = is_image_file(file)
        # is_video æ ‡å¿—åœ¨ä¸Šæ–¹å·²è·å–
        can_do_similarity = is_img and args.include_similar and args.deduplicate

        file_was_deleted_as_similar = False # æ ‡è®°å½“å‰æ–‡ä»¶æ˜¯å¦å› ç›¸ä¼¼è€Œè¢«åˆ é™¤

        if can_do_similarity:
            # æ–‡ä»¶æ˜¯å›¾ç‰‡ï¼Œå¹¶ä¸”å¯ç”¨äº†ç›¸ä¼¼åº¦æ£€æŸ¥å’Œå»é‡
            file_phash = calculate_phash(file, phash_cache, phash_cache_lock)

            if file_phash is not None:
                # æˆåŠŸè®¡ç®—å‡ºæ„ŸçŸ¥å“ˆå¸Œ
                found_similar = False
                with phash_list_lock:
                    # éå† phash_list çš„å‰¯æœ¬ï¼Œé¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹åŒä¸€ä¸ªåˆ—è¡¨
                    for original_file, original_phash in list(phash_list):
                         # æ£€æŸ¥åŸæ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ„ŸçŸ¥å“ˆå¸Œæœ‰æ•ˆï¼Œå¹¶ä¸”ç›¸ä¼¼åº¦åœ¨é˜ˆå€¼å†…
                         if original_phash is not None and original_file.exists() and abs(file_phash - original_phash) <= args.hash_threshold:
                            found_similar = True
                            logging.debug(f"ç›¸ä¼¼æ–‡ä»¶ {file} ä¸ {original_file} åŒ¹é…ï¼Œè¿›è¡Œå¤„ç†...")

                            # å¤„ç†ç›¸ä¼¼å¯¹ï¼Œå¹¶æ£€æŸ¥å½“å‰æ–‡ä»¶æ˜¯å¦è¢«åˆ é™¤
                            # handle_similar_images è¿”å› True å¦‚æœå½“å‰æ–‡ä»¶è¢«åˆ é™¤ï¼ŒFalse å¦‚æœåŸæ–‡ä»¶è¢«åˆ é™¤
                            # handle_similar_images ä¼šè´Ÿè´£åœ¨é”å†…æ›´æ–° phash_list
                            file_was_deleted_as_similar = handle_similar_images(file, original_file, file_phash, original_phash, args, source_dir_arg, phash_list, phash_list_lock)

                            # æ‰¾åˆ°äº†ç›¸ä¼¼åŒ¹é…å¹¶å¤„ç†äº†ï¼Œé€€å‡ºç›¸ä¼¼åˆ—è¡¨çš„æ£€æŸ¥å¾ªç¯
                            break

                # åœ¨æ£€æŸ¥å®Œ phash_list å
                if not found_similar:
                    # å¦‚æœæ²¡æœ‰åœ¨ phash_list ä¸­æ‰¾åˆ°ç›¸ä¼¼æ–‡ä»¶
                    # å°†å½“å‰æ–‡ä»¶çš„æ„ŸçŸ¥å“ˆå¸Œæ·»åŠ åˆ° phash_list ä¾›åç»­æ–‡ä»¶æ¯”è¾ƒ
                    # ç¡®ä¿æ˜¯å›¾ç‰‡ä¸”å½“å‰æ–‡ä»¶æ²¡æœ‰è¢«åˆ é™¤ï¼ˆfile_was_deleted_as_similar ä¸º Falseï¼‰
                    # is_img, args.include_similar, args.deduplicate æ­¤æ—¶åº”ä¸º True
                    with phash_list_lock: # è·å–é”æ¥ä¿®æ”¹ phash_list
                        # å†æ¬¡æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦å·²ç»ä»¥æŸç§æ–¹å¼è¢«æ·»åŠ åˆ° phash_listï¼ˆä¾‹å¦‚è¢« handle_similar_images æ·»åŠ ï¼‰
                        # å°½ç®¡ handle_similar_images åº”è¯¥åªåœ¨åŸæ–‡ä»¶è¢«åˆ é™¤æ—¶æ‰æ·»åŠ å½“å‰æ–‡ä»¶ï¼Œè¿™é‡Œå¤šä¸€å±‚æ£€æŸ¥æ›´ä¿é™©
                        if not any(f == file for f, _ in phash_list):
                             phash_list.append((file, file_phash))

                # æ³¨æ„ï¼š file_was_deleted_as_similar æ ‡å¿—åœ¨ handle_similar_images ä¸­è®¾ç½®

            # --- è¿™ä¸ª elif å¤„ç† can_do_similarity ä¸º True ä½† file_phash è®¡ç®—å¤±è´¥çš„æƒ…å†µ ---
            # å®ƒåº”è¯¥ä¸ `if file_phash is not None:` å¯¹é½
            elif is_img: # æ­¤æ—¶ can_do_similarity å¿…ä¸º True
                 logging.warning(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {file} çš„ç›¸ä¼¼æ€§æ£€æŸ¥ï¼Œå› ä¸ºæ„ŸçŸ¥å“ˆå¸Œè®¡ç®—å¤±è´¥ã€‚")
                 # æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œæ–‡ä»¶æ²¡æœ‰å› ä¸ºç›¸ä¼¼è¢«åˆ é™¤ï¼Œ file_was_deleted_as_similar ä¿æŒ False
                 file_was_deleted_as_similar = False # æ˜ç¡®è®¾ç½®ï¼Œå°½ç®¡é»˜è®¤æ˜¯ False

        # --- è¿™ä¸ª else å¤„ç† can_do_similarity ä¸º False çš„æƒ…å†µ ---
        # å®ƒåº”è¯¥ä¸ `if can_do_similarity:` å¯¹é½
        else:
            # æ–‡ä»¶ä¸å‚ä¸ç›¸ä¼¼åº¦æ¯”è¾ƒ (ä¾‹å¦‚ï¼šè§†é¢‘æ–‡ä»¶ï¼Œæˆ–è€…æ²¡æœ‰å¯ç”¨ç›¸ä¼¼åº¦æ£€æŸ¥ï¼Œæˆ–è€…æ²¡æœ‰å¯ç”¨å»é‡)
            # æ–‡ä»¶æ²¡æœ‰å› ä¸ºç›¸ä¼¼è¢«åˆ é™¤ï¼Œ file_was_deleted_as_similar ä¿æŒ False
            file_was_deleted_as_similar = False # æ˜ç¡®è®¾ç½®ï¼Œå°½ç®¡é»˜è®¤æ˜¯ False


        # 4. æ ¹æ®å‰é¢çš„æ£€æŸ¥ç»“æœï¼Œå†³å®šæœ€ç»ˆæ“ä½œ (å¤‡ä»½æˆ–è·³è¿‡å¤‡ä»½)
        # å¦‚æœæ–‡ä»¶å› ä¸ºç²¾ç¡®é‡å¤æˆ–ç›¸ä¼¼è¢«åˆ é™¤ï¼Œç›¸åº”çš„å¤„ç†å‡½æ•°å·²ç»å®Œæˆäº†åˆ é™¤æ“ä½œå¹¶è¿”å›äº†åˆ é™¤æ ‡å¿—
        # å¦‚æœ file_was_deleted_as_similar ä¸º Trueï¼Œè¯´æ˜å½“å‰æ–‡ä»¶åœ¨ç›¸ä¼¼åº¦æ£€æŸ¥é˜¶æ®µè¢«åˆ é™¤äº†
        if file_was_deleted_as_similar:
             # å½“å‰æ–‡ä»¶å› ä¸ºç›¸ä¼¼è€Œè¢«åˆ é™¤äº†
             return 1 # è¿”å›åˆ é™¤è®¡æ•° 1
        # å¦‚æœæ–‡ä»¶æ²¡æœ‰è¢«åˆ é™¤ (ç²¾ç¡®é‡å¤æ—¶è¢«ä¿ç•™ï¼Œæˆ–è€…ä¸æ˜¯é‡å¤/ç›¸ä¼¼æ–‡ä»¶ï¼Œæˆ–è€…ç›¸ä¼¼æ—¶è¢«ä¿ç•™)
        # å¹¶ä¸”æ²¡æœ‰è®¾ç½® --deduplicate-only (å³éœ€è¦å¤‡ä»½éé‡å¤æ–‡ä»¶)
        elif not args.deduplicate_only:
            # å¤‡ä»½è¯¥æ–‡ä»¶
            backup_file(file, args.perform_actions, Path(args.backup_dir), args.source_dir, args.simple_backup, args.simple_backup_path, overwrite_files=args.overwrite)
            # æ–‡ä»¶æœªè¢«åˆ é™¤
            return 0
        else: # æ–‡ä»¶æœªè¢«åˆ é™¤ï¼Œä½†è®¾ç½®äº† --deduplicate-only
            # ä¸å¤‡ä»½éé‡å¤æ–‡ä»¶
            return 0 # æ–‡ä»¶æœªè¢«åˆ é™¤


    except Exception as e:
         logging.error(f"âŒ å¤„ç†æ–‡ä»¶ {file} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
         return 0 # å‘ç”Ÿé”™è¯¯ï¼Œæœªåˆ é™¤æ–‡ä»¶

# ... (æ‚¨çš„å…¶ä½™ä»£ç ï¼ŒåŒ…æ‹¬ process_directory å’Œ parse_args ä¿æŒä¸å˜) ...

# ===== ç›®å½•æ‰«æå’Œå¤„ç†å‡½æ•° =====
# ===== ç›®å½•æ‰«æå’Œå¤„ç†å‡½æ•° =====
def process_directory(args, source_dir):
    """æ‰«æç›®å½•å¹¶å¤„ç†æ–‡ä»¶"""
    global interrupted # <--- æ·»åŠ è¿™ä¸€è¡Œ

    # ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„å­—å…¸å’Œåˆ—è¡¨ï¼Œå¹¶ä½¿ç”¨é”è¿›è¡ŒåŒæ­¥
    seen_hashes = {} # {file_hash: first_file_path} å­˜å‚¨æ–‡ä»¶å“ˆå¸Œå’Œç¬¬ä¸€æ¬¡é‡åˆ°çš„æ–‡ä»¶è·¯å¾„
    phash_cache = {} # {file_path: phash_value} å­˜å‚¨æ–‡ä»¶çš„æ„ŸçŸ¥å“ˆå¸Œç¼“å­˜
    phash_list = []  # [(file_path, phash_value)] å­˜å‚¨å›¾ç‰‡çš„æ„ŸçŸ¥å“ˆå¸Œåˆ—è¡¨ï¼Œç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ

    # åˆ›å»ºé”å¯¹è±¡
    seen_hashes_lock = threading.Lock()
    phash_cache_lock = threading.Lock()
    phash_list_lock = threading.Lock()


    # è¿‡æ»¤æ–‡ä»¶
    logging.info(f"ğŸ” æ‰«æç›®å½•: {source_dir}")
    all_files = []
    for f in source_dir.rglob('*'):
        if interrupted:
             logging.info("ğŸ›‘ æ‰«æç›®å½•æ—¶æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢æ‰«æã€‚")
             break
        if f.is_file():
            try:
                # æ£€æŸ¥æ–‡ä»¶å¤§å°å’Œæ‰©å±•å
                min_size_bytes = args.min_size * 1024
                # åªæœ‰å¤§å°ç¬¦åˆä¸”æ‰©å±•åç¬¦åˆçš„æ–‡ä»¶æ‰åŠ å…¥å¾…å¤„ç†åˆ—è¡¨
                if f.stat().st_size >= min_size_bytes and f.suffix.lower() in IMAGE_EXTENSIONS:
                     # æ£€æŸ¥å†™æƒé™ï¼Œå¦‚æœä¸èƒ½å†™ï¼Œé€šå¸¸ä¹Ÿä¸èƒ½åˆ é™¤æˆ–ç§»åŠ¨
                     if os.access(f, os.W_OK):
                         all_files.append(f)
                     else:
                         logging.warning(f"âš ï¸ æ–‡ä»¶æ— å†™å…¥æƒé™ï¼Œè·³è¿‡: {f}")
                # else: æ–‡ä»¶å¤§å°ä¸ç¬¦åˆæˆ–æ‰©å±•åä¸ç¬¦åˆï¼Œè·³è¿‡

            except FileNotFoundError:
                logging.debug(f"æ‰«ææ—¶æ–‡ä»¶æœªæ‰¾åˆ°: {f}") # å¯èƒ½æ˜¯æ–‡ä»¶è¢«åˆ é™¤ï¼Œæ­£å¸¸æƒ…å†µ
            except OSError as e:
                 logging.error(f"âŒ æ‰«ææ–‡ä»¶æ—¶å‘ç”Ÿ OS é”™è¯¯: {f}ï¼ŒåŸå› : {e}")
            except Exception as e:
                 logging.error(f"âŒ æ‰«ææ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {f}ï¼ŒåŸå› : {e}")


    scanned_count = len(all_files)
    # deleted_count å’Œ retained_count åœ¨å¤„ç†å¾ªç¯ä¸­ç´¯åŠ æˆ–åœ¨æœ€åè®¡ç®—
    # deleted_count = 0
    # retained_count = 0

    logging.info(f"å…±æ‰¾åˆ° {scanned_count} ä¸ªç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶ (æœ€å°å¤§å°: {args.min_size} KB, åŒ…å«è§†é¢‘: {args.include_videos})")

    # å¦‚æœéœ€è¦æ£€æµ‹ç›¸ä¼¼å›¾ç‰‡å¹¶å»é‡ï¼Œé¢„å…ˆè®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ
    # åªæœ‰å½“ --include-similar å’Œ --deduplicate åŒæ—¶å¯ç”¨æ—¶æ‰è¿›è¡Œphashè®¡ç®—
    if args.include_similar and args.deduplicate:
        logging.info("\tğŸ¨ é¢„å…ˆè®¡ç®—æ„ŸçŸ¥å“ˆå¸Œ...")
        # å¯ä»¥ä½¿ç”¨çº¿ç¨‹æ± åŠ é€Ÿ phash è®¡ç®—
        with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
            # ä½¿ç”¨ partial å°†é”å’Œå…¶ä»–ä¸å˜å‚æ•°ä¼ é€’ç»™ calculate_phash
            from functools import partial
            # åªæœ‰æ˜¯å›¾ç‰‡çš„æ–‡ä»¶æ‰éœ€è¦è®¡ç®— phash
            image_files_for_phash = [f for f in all_files if is_image_file(f)]
            futures = {executor.submit(calculate_phash_partial, file): file for file in image_files_for_phash}

            processed_phash_count = 0
            total_image_files = len(image_files_for_phash)
            print(f"\tğŸ¨ æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—è¿›åº¦: 0/{total_image_files}", end="", flush=True) # åˆå§‹åŒ–è¿›åº¦æ¡

            for future in concurrent.futures.as_completed(futures):
                if interrupted:
                     print("\nğŸ›‘ é¢„è®¡ç®—æ„ŸçŸ¥å“ˆå¸Œæ—¶æ”¶åˆ°ä¸­æ–­ä¿¡å·ã€‚æ­£åœ¨å°è¯•å…³é—­çº¿ç¨‹...")
                     executor.shutdown(wait=False, cancel_futures=True) # å°è¯•å–æ¶ˆå‰©ä½™ä»»åŠ¡å¹¶ç«‹å³è¿”å›
                     break # é€€å‡ºç»“æœæ”¶é›†å¾ªç¯
                file = futures[future]
                try:
                    # Get result to catch exceptions in worker threads
                    future.result()
                    processed_phash_count += 1
                    print(f"\t\rğŸ¨ æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—è¿›åº¦: {processed_phash_count}/{total_image_files}", end="", flush=True)
                except Exception as e:
                    logging.error(f"âŒ é¢„è®¡ç®—æ–‡ä»¶ {file} çš„æ„ŸçŸ¥å“ˆå¸Œå¤±è´¥: {e}")

            # å¦‚æœæ²¡æœ‰ä¸­æ–­ï¼Œæ‰“å°å®Œæˆä¿¡æ¯
            if not interrupted:
                 print("\n\tâœ… æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—å®Œæˆ.")
            else:
                 print("\n\tâš ï¸ æ„ŸçŸ¥å“ˆå¸Œè®¡ç®—è¢«ä¸­æ–­.")

        # æ„å»ºåˆå§‹çš„ phash_listï¼ŒåªåŒ…å«æˆåŠŸè®¡ç®—å‡º phash çš„å›¾ç‰‡æ–‡ä»¶
        with phash_cache_lock: # è·å–é”æ¥å®‰å…¨è®¿é—® phash_cache
             # è¿‡æ»¤æ‰ phash ä¸º None çš„é¡¹
             phash_list[:] = [(f, h) for f, h in phash_cache.items() if h is not None]
        logging.info(f"âœ¨ å®Œæˆæ„ŸçŸ¥å“ˆå¸Œé¢„è®¡ç®—ï¼Œå…±è·å–åˆ° {len(phash_list)} ä¸ªæ–‡ä»¶çš„æ„ŸçŸ¥å“ˆå¸Œç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒã€‚")


    # ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†æ–‡ä»¶å»é‡å’Œå¤‡ä»½
    logging.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶ ({args.threads} çº¿ç¨‹)...")
    processed_count = 0
    deleted_in_processing = 0 # Counter for files deleted during processing
    # retained_in_processing = 0 # å¯ä»¥åœ¨å¾ªç¯å†…è®¡ç®—ï¼Œæ— éœ€å•ç‹¬çš„è®¡æ•°å™¨

    with concurrent.futures.ThreadPoolExecutor(max_workers=args.threads) as executor:
        # æäº¤æ‰€æœ‰æ–‡ä»¶è¿›è¡Œå¤„ç†
        futures = {
            executor.submit(process_file, file, seen_hashes, phash_cache, phash_list, args, source_dir,
                           seen_hashes_lock, phash_cache_lock, phash_list_lock): file
            for file in all_files
        }

        try:
            for future in concurrent.futures.as_completed(futures):
                if interrupted:
                    # å¦‚æœåœ¨ç­‰å¾… future å®Œæˆæ—¶æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œå°è¯•åœæ­¢æ‰§è¡Œ
                    print("\nğŸ›‘ æ–‡ä»¶å¤„ç†æ—¶æ”¶åˆ°ä¸­æ–­ä¿¡å·ã€‚æ­£åœ¨ç­‰å¾…çº¿ç¨‹å®Œæˆå½“å‰ä»»åŠ¡...")
                    break # é€€å‡ºç»“æœæ”¶é›†å¾ªç¯

                file = futures[future]

                try:
                    # Get the result - it's the number of files deleted by this processing step (either 0 or 1)
                    deleted_by_thread = future.result()
                    deleted_in_processing += deleted_by_thread
                    processed_count += 1

                    # --- åœ¨è¿™é‡Œè®¡ç®—å½“å‰å·²å¤„ç†ä¸”æœªè¢«åˆ é™¤çš„æ–‡ä»¶æ•°é‡ ---
                    retained_so_far = processed_count - deleted_in_processing
                    # --------------------------------------------------

                    # æ›´æ–°è¿›åº¦æ¡
                    # å°† å·²ä¿ç•™: {retained_so_far} åŠ å…¥è¾“å‡º
                    print(f"\t\rğŸ“ å¤„ç†è¿›åº¦: {processed_count}/{scanned_count}, å·²åˆ é™¤: {deleted_in_processing}, å·²ä¿ç•™: {retained_so_far}", end="", flush=True)

                    # å¿ƒè·³æ—¥å¿—ï¼Œæ¯å¤„ç†ä¸€å®šæ•°é‡çš„æ–‡ä»¶è®°å½•ä¸€æ¬¡
                    if processed_count % 100 == 0:
                        logging.info(f"ğŸ’– å¿ƒè·³ - å·²å¤„ç† {processed_count}/{scanned_count} ä¸ªæ–‡ä»¶, å·²åˆ é™¤ {deleted_in_processing} ä¸ª, å·²ä¿ç•™ {retained_so_far} ä¸ª") # æ—¥å¿—ä¸­ä¹ŸåŠ å…¥ä¿ç•™æ•°

                except concurrent.futures.CancelledError:
                     logging.debug(f"æ–‡ä»¶ {file} çš„å¤„ç†ä»»åŠ¡è¢«å–æ¶ˆã€‚")
                     processed_count += 1 # ä»ç„¶ç®—ä½œä¸€ä¸ªå·²å¤„ç†æ–‡ä»¶ (è·³è¿‡å¤„ç†)
                except Exception as e:
                    logging.error(f"âŒ æ–‡ä»¶ {file} çš„å¤„ç†çº¿ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
                    processed_count += 1 # å‘ç”Ÿé”™è¯¯ï¼Œä»ç„¶ç®—ä½œä¸€ä¸ªå·²å¤„ç†æ–‡ä»¶ (æœªåˆ é™¤)

        except KeyboardInterrupt:
             print("\nğŸ›‘ è„šæœ¬ä¸»çº¿ç¨‹æ•è·åˆ°ä¸­æ–­ä¿¡å·ã€‚æ­£åœ¨é€šçŸ¥å·¥ä½œçº¿ç¨‹å¹¶ç­‰å¾…é€€å‡º...")
             interrupted = True
             pass # Allow the 'with' block to clean up

    print("\n") # åœ¨è¿›åº¦æ¡å®Œæˆåæ‰“å°æ¢è¡Œç¬¦ï¼Œç¡®ä¿ä¸‹ä¸€è¡Œè¾“å‡ºæ­£å¸¸

    # åœ¨æ‰€æœ‰çº¿ç¨‹å®Œæˆåè®¡ç®—æœ€ç»ˆç»Ÿè®¡
    deleted_count = deleted_in_processing
    # ä¿ç•™æ–‡ä»¶æ€»æ•° = æ‰«æçš„æ–‡ä»¶æ€»æ•° - å®é™…è¢«åˆ é™¤çš„æ–‡ä»¶æ€»æ•°
    retained_count = scanned_count - deleted_count

    logging.info(f"âœ… ç›®å½•å¤„ç†å®Œæˆ: {source_dir}")
    logging.info(f"   æ‰«ææ–‡ä»¶æ€»æ•°: {scanned_count}")
    logging.info(f"   åˆ é™¤æ–‡ä»¶æ€»æ•°: {deleted_count}")
    logging.info(f"   ä¿ç•™æ–‡ä»¶æ€»æ•°: {retained_count}")


    return scanned_count, retained_count, deleted_count


# ===== å‘½ä»¤è¡Œå‚æ•°è§£æ =====
def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å›¾ç‰‡å’Œè§†é¢‘å»é‡å¤‡ä»½è„šæœ¬ (é»˜è®¤æ¨¡æ‹Ÿæ‰§è¡Œ)")
    parser.add_argument("source_dir", nargs='?', type=str, help="è¦å¤„ç†çš„ç¬¬ä¸€ä¸ªå›¾ç‰‡å’Œè§†é¢‘ç›®å½• (ç•™ç©ºåˆ™æç¤ºè¾“å…¥)")
    parser.add_argument("--optional-source-dir", nargs='?', type=str, help="å¯é€‰çš„ç¬¬äºŒä¸ªå›¾ç‰‡å’Œè§†é¢‘ç›®å½•")
    parser.add_argument("backup_dir", nargs='?', type=str, help="è¢«åˆ é™¤æ–‡ä»¶çš„å¤‡ä»½ç›®å½• (ç•™ç©ºåˆ™æç¤ºè¾“å…¥)")
    parser.add_argument("-e", "--execute", dest='perform_actions', action="store_true", help="æ‰§è¡Œå®é™…çš„åˆ é™¤å’Œå¤‡ä»½æ“ä½œ")
    parser.add_argument("--include-similar", action="store_true", help="å¯ç”¨æ„ŸçŸ¥å“ˆå¸Œæ¯”å¯¹ï¼Œåˆ é™¤ç›¸ä¼¼å›¾ç‰‡")
    parser.add_argument("-log", action="store_true", help="å°†æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°")
    parser.add_argument("--delete-soft", action="store_true", help="å¯ç”¨è½¯åˆ é™¤ï¼ˆç§»åŠ¨åˆ°æŒ‡å®šç›®å½•ï¼‰")
    parser.add_argument("--trash-dir", nargs='?', type=str, help="æŒ‡å®šè½¯åˆ é™¤ç›®å½• (ç•™ç©ºåˆ™æç¤ºè¾“å…¥)")
    parser.add_argument("--log-dir", nargs='?', type=str, help="æŒ‡å®šæ—¥å¿—æ–‡ä»¶è¾“å‡ºç›®å½• (ç•™ç©ºåˆ™æç¤ºè¾“å…¥)")
    parser.add_argument("--hash-threshold", type=int, default=HASH_THRESHOLD, help=f"ç›¸ä¼¼å›¾ç‰‡å“ˆå¸Œå€¼é˜ˆå€¼ (é»˜è®¤: {HASH_THRESHOLD})")
    parser.add_argument("--threads", type=int, default=4, help="è®¾ç½®å¤„ç†çº¿ç¨‹æ•° (é»˜è®¤: 4)")
    parser.add_argument("--prefer-resolution", action="store_true", help="å¯¹äºç›¸ä¼¼å›¾ç‰‡ï¼Œä¼˜å…ˆä¿ç•™åˆ†è¾¨ç‡æ›´é«˜çš„ç‰ˆæœ¬")
    parser.add_argument("-m", "--min-size", type=int, default=DEFAULT_MIN_SIZE_KB, help=f"è®¾ç½®æœ€å°æ‰«ææ–‡ä»¶å¤§å° (KB, é»˜è®¤: {DEFAULT_MIN_SIZE_KB} KB)")
    parser.add_argument("-v", "--include-videos", action="store_true", help="åŒ…å«è§†é¢‘æ–‡ä»¶è¿›è¡Œæ£€æµ‹ (æ”¯æŒ .mp4, .avi, .mov, .mkv)")
    parser.add_argument("-s", "--simple-backup", action="store_true", help="å¯ç”¨ç®€å•å¤‡ä»½æ¨¡å¼ï¼šä»…ä¿ç•™åŸå§‹æ–‡ä»¶åï¼Œç›¸åŒæ–‡ä»¶åæ—¶æŒ‰æºæ–‡ä»¶è·¯å¾„å¤‡ä»½")
    parser.add_argument("-s1", "--simple-backup-path", action="store_true", help="å¯ç”¨å¤‡ä»½æ¨¡å¼ï¼šåŸå§‹è·¯å¾„+åŸå§‹æ–‡ä»¶å") # ä¿®æ­£å‚æ•°åä¸º simple-backup-path
    parser.add_argument("--overwrite", action="store_true", help="è¦†ç›–ç°æœ‰å¤‡ä»½æ–‡ä»¶ (æ³¨æ„ï¼šä¸è¦†ç›–æ•´ä¸ªç›®å½•)") # æ˜ç¡®è¯´æ˜ä¸è¦†ç›–æ•´ä¸ªç›®å½•
    parser.add_argument("--deduplicate", action="store_true", help="å¯ç”¨é‡å¤æ–‡ä»¶æŸ¥æ‰¾å’Œåˆ é™¤ (åŒ…æ‹¬ç²¾ç¡®é‡å¤å’Œç›¸ä¼¼å›¾ç‰‡ï¼Œå¦‚æœå¯ç”¨ç›¸ä¼¼åº¦)")
    parser.add_argument("--deduplicate-only", action="store_true", help="åªæŸ¥æ‰¾é‡å¤æ–‡ä»¶å¹¶åˆ é™¤ï¼Œä¸å¤‡ä»½éé‡å¤æ–‡ä»¶")
    return parser.parse_args()

# ===== ä¸»ç¨‹åºå…¥å£ =====
if __name__ == "__main__":
    args = parse_args()

    source_directories = []

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæºç›®å½•
    if not args.source_dir:
        args.source_dir = input("\n\tè¯·è¾“å…¥è¦å¤„ç†çš„ç¬¬ä¸€ä¸ªå›¾ç‰‡å’Œè§†é¢‘ç›®å½•: ")
    source_directory_1 = Path(args.source_dir)
    if not source_directory_1.exists():
         print(f"âŒ é”™è¯¯: æºç›®å½•ä¸å­˜åœ¨: {source_directory_1}")
         sys.exit(1)
    source_directories.append(source_directory_1)


    if args.optional_source_dir:
        source_directory_2 = Path(args.optional_source_dir)
        if not source_directory_2.exists():
             print(f"âŒ é”™è¯¯: å¯é€‰æºç›®å½•ä¸å­˜åœ¨: {source_directory_2}")
        else:
             source_directories.append(source_directory_2)

    # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨ä¸”å¯å†™
    if not args.backup_dir:
        args.backup_dir = input("\tè¯·è¾“å…¥è¢«åˆ é™¤æ–‡ä»¶çš„å¤‡ä»½ç›®å½•: ")
    backup_directory = Path(args.backup_dir)
    try:
        backup_directory.mkdir(parents=True, exist_ok=True)
    except Exception as e:
         print(f"âŒ é”™è¯¯: æ— æ³•åˆ›å»ºæˆ–è®¿é—®å¤‡ä»½ç›®å½• {backup_directory}ï¼ŒåŸå› : {e}")
         sys.exit(1)


    perform_actions = args.perform_actions
    include_similar = args.include_similar
    enable_console_log = args.log
    delete_soft = args.delete_soft
    num_threads = args.threads
    prefer_resolution = args.prefer_resolution
    min_size_kb = args.min_size
    include_videos = args.include_videos
    simple_backup = args.simple_backup
    simple_backup_with_path = args.simple_backup_path # ä½¿ç”¨ä¿®æ­£åçš„å‚æ•°å
    overwrite_files = args.overwrite
    deduplicate = args.deduplicate
    deduplicate_only = args.deduplicate_only

    trash_dir = args.trash_dir
    if delete_soft and not trash_dir:
        trash_dir = input("\tè¯·è¾“å…¥è½¯åˆ é™¤ç›®å½•: ")
    trash_directory = Path(trash_dir) if trash_dir else None
    if delete_soft and trash_directory and not trash_directory.exists():
         try:
             trash_directory.mkdir(parents=True, exist_ok=True)
         except Exception as e:
              print(f"âŒ é”™è¯¯: æ— æ³•åˆ›å»ºæˆ–è®¿é—®è½¯åˆ é™¤ç›®å½• {trash_directory}ï¼ŒåŸå› : {e}")
              # å¦‚æœè½¯åˆ é™¤ç›®å½•æ— æ³•åˆ›å»ºï¼Œç¦ç”¨è½¯åˆ é™¤
              logging.error(f"âŒ æ— æ³•åˆ›å»ºè½¯åˆ é™¤ç›®å½• {trash_directory}ï¼Œç¦ç”¨è½¯åˆ é™¤ã€‚åŸå› : {e}")
              delete_soft = False
              trash_directory = None


    # --- Log file naming modification starts here ---
    log_dir = args.log_dir
    log_base_name = "photo_dedup"
    log_extension = ".log"
    log_parent_dir = None

    if log_dir:
        log_parent_dir = Path(log_dir)
        try:
            log_parent_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åˆ›å»ºæˆ–è®¿é—®æ—¥å¿—ç›®å½• {log_parent_dir}ï¼ŒåŸå› : {e}. å°†æ—¥å¿—å†™å…¥å¤‡ä»½ç›®å½•ã€‚")
            # Fallback to backup directory for logs if specified log_dir fails
            log_parent_dir = backup_directory
            try:
                log_parent_dir.mkdir(parents=True, exist_ok=True) # Ensure backup dir is writable
            except Exception as e_backup:
                print(f"âŒ é”™è¯¯: æ— æ³•åˆ›å»ºæˆ–è®¿é—®å¤‡ä»½ç›®å½• {log_parent_dir} ä»¥å†™å…¥æ—¥å¿—ï¼ŒåŸå› : {e_backup}. è„šæœ¬å°†é€€å‡ºã€‚")
                sys.exit(1) # Exit if cannot write logs to backup dir either
    else:
        # If no log_dir is specified, use the backup directory
        log_parent_dir = backup_directory
        try:
            log_parent_dir.mkdir(parents=True, exist_ok=True) # Ensure backup dir is writable
        except Exception as e:
            print(f"âŒ é”™è¯¯: æ— æ³•åˆ›å»ºæˆ–è®¿é—®å¤‡ä»½ç›®å½• {log_parent_dir} ä»¥å†™å…¥æ—¥å¿—ï¼ŒåŸå› : {e}. è„šæœ¬å°†é€€å‡ºã€‚")
            sys.exit(1) # Exit if cannot write logs to backup dir

    # Find the next available log file number
    max_num = 0
    # Regex to find "photo_dedup" followed by digits, ending with ".log"
    # Also handle the base name "photo_dedup.log"
    log_file_pattern = re.compile(rf"^{re.escape(log_base_name)}(\d+)?{re.escape(log_extension)}$")

    try:
        for existing_file in log_parent_dir.iterdir():
            match = log_file_pattern.match(existing_file.name)
            if match:
                # Group 1 contains the number part, if present
                num_str = match.group(1)
                if num_str:
                    try:
                        max_num = max(max_num, int(num_str))
                    except ValueError:
                        # Should not happen with the regex, but handle defensively
                        logging.warning(f"âš ï¸ å‘ç°å¼‚å¸¸æ—¥å¿—æ–‡ä»¶åï¼Œæ— æ³•è§£æç¼–å·: {existing_file.name}")
                else:
                    # This matches the base name "photo_dedup.log" without a number.
                    # We can treat this as number 0 or 1, depending on desired behavior.
                    # To ensure numbering starts from 1 if base exists, treat base as 1
                    # or just find max of numbered ones and add 1.
                    # Let's find max of numbered ones and add 1. If no numbered ones, start with 1.
                    # So if "photo_dedup.log" exists but no numbered ones, max_num remains 0, new num is 1. Correct.
                    pass

    except Exception as e:
         print(f"âŒ é”™è¯¯: æ‰«ææ—¥å¿—ç›®å½• {log_parent_dir} æ—¶å‘ç”Ÿé”™è¯¯ï¼ŒåŸå› : {e}. å°†ä½¿ç”¨é»˜è®¤æ–‡ä»¶åã€‚")
         # Fallback to default naming if scanning fails
         log_file_path = log_parent_dir / DEFAULT_LOG_FILE
         # Configure basic logging handler to report this error
         logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')
         logging.error(f"âŒ æ‰«ææ—¥å¿—ç›®å½• {log_parent_dir} æ—¶å‘ç”Ÿé”™è¯¯ï¼ŒåŸå› : {e}. å°†ä½¿ç”¨é»˜è®¤æ–‡ä»¶å {log_file_path}.")

    # Determine the new log file name with the incremented number
    new_log_number = max_num + 1
    log_file_name = f"{log_base_name}{new_log_number}{log_extension}"
    log_file_path = log_parent_dir / log_file_name
    # --- Log file naming modification ends here ---


    # Configure more robust logging
    # Ensure the parent directory for the log file exists (already done above, but double check)
    try:
        log_file_path.parent.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        print(f"âŒ é”™è¯¯: æ— æ³•åˆ›å»ºæ—¥å¿—æ–‡ä»¶ç›®å½• {log_file_path.parent}ï¼ŒåŸå› : {e}. è¯·æ‰‹åŠ¨åˆ›å»ºæˆ–æ£€æŸ¥æƒé™ã€‚")
        sys.exit(1) # Exit if log directory cannot be created

    log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    # maxBytes set in bytes
    log_handler = RotatingFileHandler(log_file_path, maxBytes=LOG_FILE_SIZE_LIMIT_MB * 1024 * 1024, backupCount=3, encoding='utf-8') # Specify encoding
    log_handler.setFormatter(log_formatter)
    logger = logging.getLogger()
    # Set INFO or DEBUG level based on -log parameter, DEBUG for more detailed debug info
    logger.setLevel(logging.DEBUG if args.log else logging.INFO)
    # Clear potentially existing handlers to prevent duplicates
    if logger.hasHandlers():
        logger.handlers.clear()
    logger.addHandler(log_handler)

    if enable_console_log:
        # Only add StreamHandler if console logging is enabled and no StreamHandler exists
        if not any(isinstance(h, logging.StreamHandler) for h in logger.handlers):
             console_handler = logging.StreamHandler(sys.stdout)
             console_handler.setFormatter(log_formatter)
             logger.addHandler(console_handler)


    logging.info("==== è„šæœ¬å¼€å§‹è¿è¡Œ ====")
    logging.info(f"æºç›®å½•: {source_directories}")
    logging.info(f"å¤‡ä»½ç›®å½•: {backup_directory}")
    logging.info(f"è¿è¡Œæ¨¡å¼: {'æ‰§è¡Œ' if perform_actions else 'æ¨¡æ‹Ÿ'}")
    logging.info(f"å¯ç”¨é‡å¤æ–‡ä»¶æŸ¥æ‰¾å’Œåˆ é™¤ (--deduplicate): {deduplicate}")
    logging.info(f"ä»…æŸ¥æ‰¾é‡å¤æ–‡ä»¶å¹¶åˆ é™¤ (--deduplicate-only): {deduplicate_only}")
    if deduplicate:
        logging.info(f"åŒ…å«ç›¸ä¼¼å›¾ç‰‡ (--include-similar): {include_similar}")
        if include_similar:
            logging.info(f"ç›¸ä¼¼åº¦å“ˆå¸Œé˜ˆå€¼ (--hash-threshold): {args.hash_threshold}")
            logging.info(f"ä¼˜å…ˆä¿ç•™é«˜åˆ†è¾¨ç‡ (--prefer-resolution): {prefer_resolution}")
    logging.info(f"æœ€å°æ‰«ææ–‡ä»¶å¤§å° (-m): {min_size_kb} KB")
    logging.info(f"åŒ…å«è§†é¢‘æ–‡ä»¶ (-v): {include_videos}")
    logging.info(f"ç®€å•å¤‡ä»½æ¨¡å¼ (-s): {simple_backup}")
    logging.info(f"åŸå§‹è·¯å¾„å¤‡ä»½æ¨¡å¼ (-s1): {simple_backup_with_path}")
    logging.info(f"è¦†ç›–ç°æœ‰å¤‡ä»½ (--overwrite): {overwrite_files}")
    logging.info(f"è½¯åˆ é™¤ (--delete-soft): {delete_soft}")
    if delete_soft:
        logging.info(f"å›æ”¶ç«™ç›®å½• (--trash-dir): {trash_directory}")
    logging.info(f"ä½¿ç”¨çº¿ç¨‹æ•° (--threads): {num_threads}")
    logging.info(f"æ—¥å¿—æ–‡ä»¶: {log_file_path}") # Log the actual file name being used
    logging.info(f"æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å° (-log): {enable_console_log}")

    # === æ‚¨æä¾›çš„æ§åˆ¶å°è¾“å‡ºä»£ç  ===
    print(f"\tè¿è¡Œæ¨¡å¼: {'æ‰§è¡Œ' if perform_actions else 'æ¨¡æ‹Ÿ'}")
    print(f"\tåŒ…å«ç›¸ä¼¼å›¾ç‰‡: {include_similar}")
    print(f"\tç›¸ä¼¼åº¦å“ˆå¸Œé˜ˆå€¼: {args.hash_threshold}")
    print(f"\tä½¿ç”¨çº¿ç¨‹æ•°: {num_threads}")
    print(f"\tä¼˜å…ˆä¿ç•™é«˜åˆ†è¾¨ç‡: {prefer_resolution}")
    print(f"\tæœ€å°æ‰«ææ–‡ä»¶å¤§å°: {min_size_kb} KB")
    print(f"\tåŒæ—¶è¾“å‡ºæ—¥å¿—åˆ°æ§åˆ¶å°: {enable_console_log}")
    print(f"\tåŒ…å«è§†é¢‘æ–‡ä»¶: {include_videos}")
    # Determine and print the active backup mode
    active_backup_mode_print = "é»˜è®¤"
    if simple_backup:
        active_backup_mode_print = "ç®€å•(-s)"
    elif simple_backup_with_path:
        active_backup_mode_print = "åŸå§‹è·¯å¾„(-s1)"
    print(f"\tå¤‡ä»½æ¨¡å¼: {active_backup_mode_print}") # æ”¹ä¸ºåˆå¹¶è¾“å‡º
    print(f"\tè½¯åˆ é™¤: {delete_soft}")
    if delete_soft:
        print(f"\tå›æ”¶ç«™ç›®å½•: {trash_directory}")
    # =============================

    sys.stdout.flush() # æ·»åŠ è¿™ä¸€è¡Œ

    all_scanned_count = 0
    all_retained_count = 0
    all_deleted_count = 0

    try:
        for source_dir in source_directories:
            source_path = Path(source_dir)
            # åœ¨ process_directory ä¸­å·²ç»æ£€æŸ¥ç›®å½•å­˜åœ¨æ€§ï¼Œè¿™é‡Œä¸å†é‡å¤æ£€æŸ¥
            scanned_count, retained_count, deleted_count = process_directory(
                args=args,
                source_dir=source_path,
            )
            all_scanned_count += scanned_count
            all_retained_count += retained_count
            all_deleted_count += deleted_count
    except Exception as e:
         logging.critical(f"è„šæœ¬ä¸»å¾ªç¯å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
         print(f"\nâŒ è„šæœ¬è¿è¡Œä¸­æ–­ï¼Œå‘ç”Ÿè‡´å‘½é”™è¯¯: {e}")
    except KeyboardInterrupt:
         # å·²åœ¨ signal_handler ä¸­å¤„ç†ï¼Œè¿™é‡Œæ•è·æ˜¯ä¸ºäº†é˜²æ­¢æ„å¤–
         print("\nğŸ›‘ è„šæœ¬è¢«ç”¨æˆ·ä¸­æ–­ã€‚")
         logging.info("==== è„šæœ¬è¢«ç”¨æˆ·ä¸­æ–­ ====")
    finally:
        # æœ€ç»ˆç»Ÿè®¡
        logging.info("\t==== è„šæœ¬è¿è¡Œç»“æŸ ====")
        logging.info(f"\tæ‰€æœ‰æºç›®å½•ç»Ÿè®¡ï¼š")
        logging.info(f"\t   æ‰«ææ–‡ä»¶æ€»æ•°: {all_scanned_count}")
        logging.info(f"\t   åˆ é™¤æ–‡ä»¶æ€»æ•°: {all_deleted_count}")
        logging.info(f"\t   ä¿ç•™æ–‡ä»¶æ€»æ•°: {all_retained_count}")
        print(f"\n==== è„šæœ¬è¿è¡Œç»“æŸ ====")
        print(f"æ‰€æœ‰æºç›®å½•ç»Ÿè®¡ï¼š")
        print(f"\t   æ‰«ææ–‡ä»¶æ€»æ•°: {all_scanned_count}")
        print(f"\t   åˆ é™¤æ–‡ä»¶æ€»æ•°: {all_deleted_count}")
        print(f"\t   ä¿ç•™æ–‡ä»¶æ€»æ•°: {all_retained_count}\n")
